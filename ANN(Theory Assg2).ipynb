{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression as maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulation of logistic regression as maximisation of a negetive log likelihood function by taking the output from the last layer and using it to get a probability distribution for some classes ({0,1} in case of binary classification).\n",
    "                The goal of using gradient descent is to optimize the weights to minimize the cost function(sum) over the input data by minimizing the error term averaged over i.e $ (\\sum_{i=1}^{m} (log loss-y_i))/m$  doing this helps us minimize the error (as gradients poinnts in the direction of mazimum descent thus decreasing the error value on every iteration,\n",
    "                A binary output cannot be given (instead a class probability is give) as gradient descent algorithm donot converge or tak a very long time as the slope of the sigmoid function used to calculate $y_{hat}$ is very small as we reach close to the desired value and thus giving the probability for classes is the best thing to do.For non binary classification we use softmax funtion toclassify the data.\n",
    "                We are not using the loss function on the probabilities as to use the loss function we need to find an error term and the probabilities that we are taking in this case is aren't realted to y_i then they dont make sense as there must be something whose probability of being soething else is being found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Gradient descent with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent with momentum is an optimisation algorithm used as it works faster as compared to general gradient descent.\n",
    "What we do is to calculate the exponentially weighted averages for your gradients and then update your weights with the new values.As the gradients previously claculated also pointed in the direction of maximum decease of the cost(error) so taking the exponential averages basically means taking average over $1/(1-beta)$ previous values,where beta is a hyperparamter.\n",
    "The update term is:\n",
    "vdW = 0, vdb = 0\n",
    "on iteration t:\n",
    "\t# can be mini-batch or batch gradient descent\n",
    "\tcompute dw, db on current mini-batch                \n",
    "\t\t\t\n",
    "\t vdW = beta * vdW + (1 - beta) * dW \n",
    "     vdb = beta * vdb + (1 - beta) * db \n",
    "\t W = W - learning_rate * vdW \n",
    "\t b = b - learning_rate * vdb \n",
    "\n",
    "and then use them to update the weights and biases,The physucl intuition is that we are finding the gradient taking care of the  previous gradient values or can be thought as W,b being the velocity and the other terms being the acceleration and iteting over t iterarions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Brief questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.Why mini batches?\n",
    "Mini batches are used as when we are training machine learning models which include usage of large amount of data then the iteration process slows down a lot, oin order to avoid that we use batches and update the cost function according to a batch aand then find the average over all the batches,the basic idea is to do gradient descent some times even before we havetaken input of all the data.Like for normal gradient descent we use all or 1 input at a time while in mini batch gradient descent we use a mini batch with 1-m inputs at a time and at the end of a mini batch we update the weights which helps the model learn faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.Non symmetric weight initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the weights are intialised symetrically then as we are dealing with fully connected layers, all the resulta from each layer of neurons will give the same output and if outputs are same then the gradients would also be the same,thus the weights woud be updates by the same amounts and thus model will essentially contain input layer and some unnecessary hidden layers aas they output the same values,thus the model would not be accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.Regularisationn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train the model using the training data sometimes the cost function becomes too complex ,might be due to excessive training or more number of input features then required ,to deal with these we need to regularise our model when it is overfitting.\n",
    "              The basic idea is that we need to simplify the model a bit and thus we add a regularisation term to the cost function so we choose lambda such that it neglect some of the w(s).\n",
    "Some other ways to get rid of overfitting are:\n",
    "\n",
    "1.Data augmentation-Using image data the using the same data by flipping the images andd rotating the image as by doing so the array change and thus the input order change.\n",
    "\n",
    "2.Early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d.Batch normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and get 10 times or more improvement in the training speed.\n",
    "Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift),batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.It also helps reducing overfitting.\n",
    "We  do need to do a shift before giving the normalised value before passing it to the next layer,if we want data with mean!=0 and varinace!=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Calc dim of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input image is N1*N2*3 ,generally N1 and N2 are odd numbers,Filters are f * f ,if the padding is p=1 and stride is s=2 then the output dimensions are $x,y={floor((N1-1)/2 +1),floor((N2-1)/2+1)}$ and as we have taken the output channel to be 8 so we are dealing with 8 fature maps i.e output will be $x * y * 8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
